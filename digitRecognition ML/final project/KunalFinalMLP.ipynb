{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3SnbWDTjxpCQ"
   },
   "outputs": [],
   "source": [
    "#attempt 2 for mnist data recognition\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h-fTOIHcxwQj"
   },
   "outputs": [],
   "source": [
    "##load the data \n",
    "(X_train,y_train),(X_test,y_test)= keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f0xejRKBx1K4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train = X_train.astype(np.float)/255\n",
    "# X_test = X_test.astype(np.float)/255\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uev3x2gHyB8s",
    "outputId": "1f64c68f-09a2-468a-c4d6-ef1daa7ec394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 14:20:43.381968: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-25 14:20:43.383419: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vj1AExGXyGQV"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewhoYdkyyJ27",
    "outputId": "3026ed5a-51ab-4e49-ede6-9cd50cfeeb95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/65\n",
      "48000/48000 [==============================] - 4s 88us/sample - loss: 0.3212 - accuracy: 0.9089 - val_loss: 0.1793 - val_accuracy: 0.9483\n",
      "Epoch 2/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1490 - accuracy: 0.9569 - val_loss: 0.1317 - val_accuracy: 0.9617\n",
      "Epoch 3/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1060 - accuracy: 0.9688 - val_loss: 0.1159 - val_accuracy: 0.9661\n",
      "Epoch 4/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0804 - accuracy: 0.9769 - val_loss: 0.0982 - val_accuracy: 0.9711\n",
      "Epoch 5/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.0645 - accuracy: 0.9809 - val_loss: 0.0921 - val_accuracy: 0.9724\n",
      "Epoch 6/65\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 0.0519 - accuracy: 0.9850 - val_loss: 0.0980 - val_accuracy: 0.9702\n",
      "Epoch 7/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.0420 - accuracy: 0.9881 - val_loss: 0.0873 - val_accuracy: 0.9732\n",
      "Epoch 8/65\n",
      "48000/48000 [==============================] - 4s 76us/sample - loss: 0.0346 - accuracy: 0.9906 - val_loss: 0.0922 - val_accuracy: 0.9737\n",
      "Epoch 9/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.0903 - val_accuracy: 0.9732\n",
      "Epoch 10/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.0238 - accuracy: 0.9936 - val_loss: 0.0863 - val_accuracy: 0.9737\n",
      "Epoch 11/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0204 - accuracy: 0.9948 - val_loss: 0.0876 - val_accuracy: 0.9755\n",
      "Epoch 12/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.0164 - accuracy: 0.9960 - val_loss: 0.0867 - val_accuracy: 0.9751\n",
      "Epoch 13/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0142 - accuracy: 0.9967 - val_loss: 0.0882 - val_accuracy: 0.9760\n",
      "Epoch 14/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.0997 - val_accuracy: 0.9739\n",
      "Epoch 15/65\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.0925 - val_accuracy: 0.9758\n",
      "Epoch 16/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.0965 - val_accuracy: 0.9736\n",
      "Epoch 17/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0984 - val_accuracy: 0.9754\n",
      "Epoch 18/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.1097 - val_accuracy: 0.9737\n",
      "Epoch 19/65\n",
      "48000/48000 [==============================] - 4s 74us/sample - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0998 - val_accuracy: 0.9765\n",
      "Epoch 20/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.1003 - val_accuracy: 0.9768\n",
      "Epoch 21/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.1196 - val_accuracy: 0.9738\n",
      "Epoch 22/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.1132 - val_accuracy: 0.9744\n",
      "Epoch 23/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.1037 - val_accuracy: 0.9769\n",
      "Epoch 24/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9772\n",
      "Epoch 25/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 7.8233e-04 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9772\n",
      "Epoch 26/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.1292 - val_accuracy: 0.9741\n",
      "Epoch 27/65\n",
      "48000/48000 [==============================] - 5s 112us/sample - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.1141 - val_accuracy: 0.9743\n",
      "Epoch 28/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1075 - val_accuracy: 0.9777\n",
      "Epoch 29/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 4.5071e-04 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9778\n",
      "Epoch 30/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 3.9678e-04 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9785\n",
      "Epoch 31/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 3.8512e-04 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 0.9762\n",
      "Epoch 32/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 33/65\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.1395 - val_accuracy: 0.9738\n",
      "Epoch 34/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1202 - val_accuracy: 0.9765\n",
      "Epoch 35/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 4.7500e-04 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9781\n",
      "Epoch 36/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 2.3570e-04 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9783\n",
      "Epoch 37/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.8285e-04 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9779\n",
      "Epoch 38/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 1.5584e-04 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9778\n",
      "Epoch 39/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.5600e-04 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9775\n",
      "Epoch 40/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.0153 - accuracy: 0.9952 - val_loss: 0.1293 - val_accuracy: 0.9748\n",
      "Epoch 41/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.1289 - val_accuracy: 0.9760\n",
      "Epoch 42/65\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 4.4601e-04 - accuracy: 0.9999 - val_loss: 0.1241 - val_accuracy: 0.9768\n",
      "Epoch 43/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 2.3475e-04 - accuracy: 1.0000 - val_loss: 0.1238 - val_accuracy: 0.9769\n",
      "Epoch 44/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.4454e-04 - accuracy: 1.0000 - val_loss: 0.1239 - val_accuracy: 0.9776\n",
      "Epoch 45/65\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.1500e-04 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 0.9778\n",
      "Epoch 46/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 1.0103e-04 - accuracy: 1.0000 - val_loss: 0.1256 - val_accuracy: 0.9779\n",
      "Epoch 47/65\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 8.6638e-05 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9782\n",
      "Epoch 48/65\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.1519 - val_accuracy: 0.9747\n",
      "Epoch 49/65\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1488 - val_accuracy: 0.9756\n",
      "Epoch 50/65\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 3.9837e-04 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9763\n",
      "Epoch 51/65\n",
      "48000/48000 [==============================] - 4s 75us/sample - loss: 1.2702e-04 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9766\n",
      "Epoch 52/65\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 9.5308e-05 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9767\n",
      "Epoch 53/65\n",
      "17536/48000 [=========>....................] - ETA: 1s - loss: 7.0199e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "his = model.fit(X_train,y_train,batch_size=64,epochs=65,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tTMgThNyeMo",
    "outputId": "f2b84c6e-67ac-453a-e82a-aaee018c5863"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "gkeyYUylz89Q",
    "outputId": "a81fb45c-8834-4982-85b2-7c29c655bad7"
   },
   "outputs": [],
   "source": [
    "# predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "finalArray = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(finalArray)\n",
    "plt.imshow(X_test[0])\n",
    "plt.title(y_test[0])\n",
    "filename = 'model.h5'\n",
    "# filename = 'offlineModel.h5'\n",
    "model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2=keras.models.load_model(\"offlineModel.h5py\")\n",
    "#this is how we load the already saved model\n",
    "# model2.evaluate(X_test,y_test)\n",
    "\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "file = \"download.png\"\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (28,28))\n",
    "image = 255-image          #inverts image. Always gets read inverted.\n",
    "\n",
    "plt.imshow(image.reshape(28, 28),cmap='Greys')\n",
    "plt.show()\n",
    "pred = model.predict(image.reshape(1, 28, 28, 1), batch_size=1)\n",
    "\n",
    "print(pred.argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "digitRecognition2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
